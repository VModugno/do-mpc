{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63570e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must run this, the one installed is missing mpi\n",
    "!pip install stable-baselines[mpi]==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a52dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.cmd_util import make_vec_env\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, VecNormalize\n",
    "\n",
    "from differential_drive_env_v2_wrappers import DifferentialDriveEnvV2Unscaled, RLAgentUnscalingWrapper\n",
    "import baseline_integration as bi\n",
    "\n",
    "from stable_baselines.gail import generate_expert_traj\n",
    "from stable_baselines.gail import ExpertDataset\n",
    "from stable_baselines.common.evaluation import evaluate_policy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c97284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "init_pose = [0.3, 0.3, np.pi]\n",
    "env = DifferentialDriveEnvV2Unscaled(init_position=init_pose,goal_threshold = 0.1, L=0.5, r=0.17, max_duration = 500)\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "env = VecNormalize(env)\n",
    "\n",
    "# name of the model to use to create the expert dataset\n",
    "# maybe one of these\n",
    "#    \"ppo2_gaussian_012\"\n",
    "#    \"ppo2_gaussian_016\"\n",
    "#    \"ppo2_gaussian_017\"\n",
    "#    \"test_corrected_env\"\n",
    "model_name = \"\"\n",
    "\n",
    "# Load the model\n",
    "model = PPO2.load(model_name)\n",
    "model.set_env(env)\n",
    "# generate_expert_traj(model,file_name,n_episodes)\n",
    "generate_expert_traj(model, 'expert', n_episodes=50)\n",
    "\n",
    "del model \n",
    "del env # (you can delete them now, not useful anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddba4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DifferentialDriveEnvV2Unscaled(init_position=init_pose,goal_threshold = 0.1, L=0.5, r=0.16, max_duration = 500)\n",
    "env = make_vec_env(lambda: env, n_envs=4)\n",
    "env = VecNormalize(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e117f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ExpertDataset(expert_path='expert.npz',\n",
    "                        traj_limitation=1, batch_size=128)\n",
    "\n",
    "# define the model to pretrain on the generated dataset\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "\n",
    "# Pretrain the PPO2 model\n",
    "model.pretrain(dataset, n_epochs=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e54dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further train the model if necessary. It may be better to detach the current environment and make one with multiple\n",
    "# environments in the make_vec function for parallelized and faster learning.\n",
    "# Uncomment the following line if you want to \n",
    "# \n",
    "# del env\n",
    "# env = DifferentialDriveEnvV2Unscaled(init_position=init_pose,goal_threshold = 0.1, L=0.5, r=0.16, max_duration = 500)\n",
    "# env = make_vec_env(lambda: env, n_envs=4)\n",
    "# env = VecNormalize(env)\n",
    "# model.set_env(env)\n",
    "\n",
    "model.learn(total_timesteps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80523823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the new model a name and save it\n",
    "pretrained_model_name = \"pretraining_attempt\"\n",
    "model.save(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090970b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model; it returns the mean reward per episode, and the std \n",
    "evaluate_policy(model,env.envs[0],n_eval_episodes = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trajectory obtained. \n",
    "\n",
    "ppo2_model_name = \"pretraining_attempt\" # Change this\n",
    "env_class_name = DifferentialDriveEnvV2Unscaled\n",
    "rl_agent_wrapper_class = RLAgentUnscalingWrapper\n",
    "rl_agent_wrapper_params = {\"state_scaling_factors\": [1.0, 1.0, np.pi], \"action_scaling_factors\": [3.0, 3.0]}\n",
    "init_robot_pose = {'x': 0.3, 'y': 0.3, 'theta': np.pi/2}\n",
    "#init_robot_pose = {'x': 0.3, 'y': 0.3, 'theta': 0}\n",
    "obss, actions = bi.load_and_run_model(ppo2_model_name, 500, 0.50, 0.16, env_class_name, list(init_robot_pose.values()), rl_agent_wrapper_class, rl_agent_wrapper_params)\n",
    "print(\"Ho {} obss e {} actions\".format(len(obss),len(actions)))\n",
    "print(\"X \\n {}\".format(obss))\n",
    "print(\"Commands \\n {}\".format(actions))\n",
    "bi.show_rl_trajectory(obss,actions,0.50,0.16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

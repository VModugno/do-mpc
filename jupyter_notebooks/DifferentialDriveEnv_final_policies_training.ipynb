{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVmYXZdL-Edw"
   },
   "outputs": [],
   "source": [
    "!-H pip3 install --upgrade pip\n",
    "!-H pip2 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNwxK4EYH62n"
   },
   "outputs": [],
   "source": [
    "# Stable Baselines only supports tensorflow 1.x for now\n",
    "#%tensorflow_version 1.x\n",
    "import sys \n",
    "!{sys.executable} -m pip install tensorflow==1.15.0\n",
    "\n",
    "!pip install stable-baselines[mpi]==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFdVUMxK45-F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Box\n",
    "from gym.utils import seeding\n",
    "\n",
    "import math\n",
    "\n",
    "model_name12 = \"ppo2_012_exponential_reward\"\n",
    "model_name16 = \"model_016\"\n",
    "model_name17 = \"model_017\"\n",
    "\n",
    "class DifferentialDriveEnv(Env):\n",
    "  \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "  # metadata = {'render.modes': ['human']}\n",
    "  metadata = {'render.modes': ['console']}\n",
    "\n",
    "  def __init__(self, L, r, delta_t = 0.01,\n",
    "               init_position=None, goal_position=[0,0],\n",
    "               goal_threshold = 0.1, max_duration = 500,\n",
    "               min_action=-1, max_action=1,\n",
    "               min_position=[-1,-1], max_position=[1,1]):\n",
    "    \n",
    "    super(DifferentialDriveEnv, self).__init__()\n",
    "\n",
    "    #Define model parameters\n",
    "    self.L = L \n",
    "    self.r = r\n",
    "\n",
    "    self.orientation_map = math.pi\n",
    "    self.action_map = 3\n",
    "\n",
    "    self.delta_t = delta_t\n",
    "    self.threshold = goal_threshold\n",
    "\n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects\n",
    "    self.min_action = min_action\n",
    "    self.max_action = max_action\n",
    "    self.min_position = min_position\n",
    "    self.max_position = max_position\n",
    "\n",
    "    self.min_orientation = -1\n",
    "    self.max_orientation = 1\n",
    "    \n",
    "    self.init_position = init_position\n",
    "    self.goal_position = goal_position\n",
    "\n",
    "    # self.goal_velocity = goal_velocity\n",
    "    # self.goal_orientation = goal_orientation\n",
    "    self.goal_reached_count = 0\n",
    "\n",
    "    self.max_duration = max_duration\n",
    "    self.duration = self.max_duration\n",
    "    #self.max_duration = 250\n",
    "    \n",
    "    self.low_state = np.array(\n",
    "        self.min_position+[self.min_orientation], dtype=np.float32\n",
    "    )\n",
    "\n",
    "    self.high_state = np.array(\n",
    "        self.max_position+[self.max_orientation], dtype=np.float32\n",
    "    )\n",
    "\n",
    "    self.viewer = None\n",
    "\n",
    "    self.action_space = Box(\n",
    "        low=self.min_action,\n",
    "        high=self.max_action,\n",
    "        shape=(2,),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    self.observation_space = Box(\n",
    "        low=self.low_state,\n",
    "        high=self.high_state,\n",
    "        shape=(3,),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    self.seed()\n",
    "    self.reset()\n",
    "\n",
    "  def seed(self, seed=None):\n",
    "    self.np_random, seed = seeding.np_random(seed)\n",
    "    return [seed]\n",
    "\n",
    "  def reset(self):\n",
    "    # Reset the state of the environment to an initial state\n",
    "\n",
    "    if self.init_position is None:\n",
    "      self.state = np.array([self.np_random.uniform(low=-0.1, high=0.1), self.np_random.uniform(low=-0.5, high=0.5), -1/2 + self.np_random.uniform(low=-0.01, high=0.01)])      \n",
    "      #self.state = np.array([self.np_random.uniform(low=-0.03, high=-0.02), self.np_random.uniform(low=-0.02, high=-0.01), -math.pi/2])\n",
    "      #self.state = np.array([self.np_random.uniform(low=0.02, high=0.03), self.np_random.uniform(low=-0.02, high=-0.01), -math.pi/2])\n",
    "      # self.state = np.array([self.np_random.uniform(low=self.min_position[0], high=self.max_position[0]), self.np_random.uniform(low=self.min_position[0], high=self.max_position[0]), math.pi/2])\n",
    "    elif isinstance(self.init_position, list):\n",
    "      if len(self.init_position) == 3:\n",
    "        self.state = np.array(self.init_position)\n",
    "      else:\n",
    "        raise Exception(\"Initial position must be size 3: [x, y, theta]\")\n",
    "    else:\n",
    "      raise Exception(\"Initial position must be a list: [x, y, theta]\")\n",
    "    #self.max_duration = 500\n",
    "    self.duration = self.max_duration\n",
    "    return np.array(self.state)\n",
    "\n",
    "  def render(self, mode='console', close=False):\n",
    "    if mode is 'console':\n",
    "      print(\"========================================================\")\n",
    "      print(\">> Pos: x = \",self.state[0],\"; y = \",self.state[1])\n",
    "      print(\">> Ori: \",self.state[2])\n",
    "      print(\"========================================================\")\n",
    "\n",
    "  def step(self, action):\n",
    "    x = self.state[0]\n",
    "    y = self.state[1]\n",
    "    theta = self.state[2]*self.orientation_map\n",
    "\n",
    "    v = self.action_map*(action[1] + action[0]) * self.r / 2      #max vlin = 2 * 3 *0.15 /2 = 3*0.15 = 0.45 m/s (was 0.75 wmax = 5)\n",
    "    w = self.action_map*(action[1] - action[0]) * self.r / self.L #max omega = 3 - (-3) * 0.15 /0.5 = 0.9 /0.5  1,8 rad/s ()\n",
    "\n",
    "    x = x + v * self.delta_t * math.cos(theta)\n",
    "    y = y + v * self.delta_t * math.sin(theta)\n",
    "    theta = theta + w * self.delta_t\n",
    "\n",
    "    if theta > math.pi:\n",
    "        theta = theta - 2*math.pi\n",
    "    elif theta < -math.pi:\n",
    "        theta = (2*math.pi + theta)\n",
    "    \n",
    "    #threshold = 0.1\n",
    "    #threshold = 0.001\n",
    "    distance_to_target = np.linalg.norm(np.array(self.goal_position)-np.array([x, y]))\n",
    "\n",
    "    goal_reached = bool(distance_to_target <= self.threshold)\n",
    "    time_expired = bool(self.duration <= 0)\n",
    "    too_far = bool(distance_to_target > 0.1)\n",
    "\n",
    "    reward = 0\n",
    "    #beta = 10\n",
    "    #sigma = 0.3\n",
    "\n",
    "    #if done:\n",
    "    if goal_reached:\n",
    "      reward += 10000.0\n",
    "      self.goal_reached_count += 1\n",
    "    else:\n",
    "      #reward -= distance_to_target*0.1\n",
    "      reward -= distance_to_target**2 + (np.arctan2(y,x) + np.pi - theta)**2\n",
    "      #reward -= (np.arctan2(y,x) + np.pi - theta)**2\n",
    "      #reward = beta*np.exp(-(distance_to_target**2)/(2*sigma**2))\n",
    "      if time_expired:\n",
    "        reward -= 500\n",
    "\n",
    "    done = goal_reached or time_expired\n",
    "\n",
    "    info = {}\n",
    "\n",
    "    self.state = np.array([x, y, theta/self.orientation_map])\n",
    "\n",
    "    self.duration -= 1\n",
    "\n",
    "    return self.state, reward, done, info\n",
    "\n",
    "  def close(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhsMwW8l3Iiu"
   },
   "outputs": [],
   "source": [
    "from stable_baselines import DQN, PPO2, A2C, ACKTR, TRPO\n",
    "from stable_baselines.common.cmd_util import make_vec_env\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "\n",
    "\n",
    "# Instantiate the env\n",
    "#env = DifferentialDriveEnv(L=50, r=15)\n",
    "\n",
    "init_pose = [0.3, 0.3, np.pi]\n",
    "env = DifferentialDriveEnv(init_position=init_pose, goal_threshold = 0.1, L=0.5, r=0.12, max_duration = 1000)\n",
    "#env = DifferentialDriveEnv(init_position=init_pose,goal_threshold = 0.05,L=0.5, r=0.16)\n",
    "#env = DifferentialDriveEnv(init_position=init_pose,goal_threshold = 0.05,L=0.5, r=0.17)\n",
    "\n",
    "# wrap it\n",
    "env = make_vec_env(lambda: env, n_envs=4)\n",
    "\n",
    "# Keep on training an pre-existing model\n",
    "#model = PPO2.load(model_name12)\n",
    "\n",
    "# Train the agent\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "#model.set_env(env)\n",
    "model.learn(total_timesteps=50000000)\n",
    "model.save(model_name12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8LNgh6C-EnP"
   },
   "outputs": [],
   "source": [
    "model.save(model_name12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0CajpGqjfHH"
   },
   "outputs": [],
   "source": [
    "print(\"Goal reached in env 0: {} times\".format(env.envs[0].goal_reached_count))\n",
    "print(\"Goal reached in env 1: {} times\".format(env.envs[1].goal_reached_count))\n",
    "print(\"Goal reached in env 2: {} times\".format(env.envs[2].goal_reached_count))\n",
    "print(\"Goal reached in env 3: {} times\".format(env.envs[3].goal_reached_count))\n",
    "\n",
    "del env\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDoVIh07jSPy"
   },
   "outputs": [],
   "source": [
    "#env = DifferentialDriveEnv(L=50, r=15)\n",
    "init_pose = [0.3, 0.3, np.pi]\n",
    "env = DifferentialDriveEnv(L=0.5, r=0.12, init_position=init_pose)\n",
    "model = PPO2.load(model_name12) #typo it should be ppo_etc\n",
    "obs = env.reset()\n",
    "while False:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render(mode = 'console')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4WJ8GYSnxLa"
   },
   "outputs": [],
   "source": [
    "def show_rl_trajectory(obs_list):\n",
    "    x_values = list(map(lambda obs: obs[0], obs_list))\n",
    "    y_values = list(map(lambda obs: obs[1], obs_list))\n",
    "    theta_values = list(map(lambda obs: obs[2], obs_list))\n",
    "    print(\"Starting point: x:{}, y:{} -PURPLE-\".format(x_values[0],y_values[0]))\n",
    "    print(\"End point: x:{}, y:{} -GREEN-\".format(x_values[-1],y_values[-1]))\n",
    "    def on_close(event):\n",
    "        print('Closed Figure!')\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    fig.canvas.mpl_connect('close_event', on_close)\n",
    "    plt.scatter(x_values, y_values,color='blue')\n",
    "    plt.scatter(x_values[0],y_values[0],color='purple')\n",
    "    plt.scatter(x_values[-1],y_values[-1],color='green')\n",
    "    plt.scatter(0,0,color='red',marker='x')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZgY4B4l3qbu"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "obs = env.reset()\n",
    "n_steps = 5000\n",
    "score = 0.0\n",
    "history = []\n",
    "obss = []\n",
    "obss.append(obs)\n",
    "history.append(score)\n",
    "for step in range(n_steps):\n",
    "  action, _ = model.predict(obs, deterministic=True)\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  print(\"Action: \", action)\n",
    "  obs, reward, done, info = env.step(action)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "  env.render(mode='console',close = True)\n",
    "  score+=float(reward)\n",
    "  history.append(score)\n",
    "  obss.append(obs)\n",
    "  if done:\n",
    "    # Note that the VecEnv resets automatically\n",
    "    # when a done signal is encountered\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break\n",
    "\n",
    "plt.plot(history)\n",
    "plt.title('Episode score')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Steps')\n",
    "plt.show()\n",
    "show_rl_trajectory(obss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ociy20YyGIdO"
   },
   "outputs": [],
   "source": [
    "del env\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DifferentialDriveEnv_final_policies_training_backup.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}